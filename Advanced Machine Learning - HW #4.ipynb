{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Machine Learning\n",
    "\n",
    "# Home Exercise #4\n",
    "\n",
    "Assignment Due: 17/05/21 23:59\n",
    "\n",
    "Jonathan Schler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "student1Name=\"Matan Mizrachi\" # student1 name here\n",
    "student1ID=\"207299363\" # student1 ID here\n",
    "\n",
    "\n",
    "student2Name=\"Ron Yosef\" # student2 name here\n",
    "student2ID=\"318466711\" # student2 ID here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this homework is to try to solve the problem of predicting wine quality from review texts and other properties of the wine. You can find the dataset here:https://www.kaggle.com/zynicide/wine-reviews\n",
    "\n",
    "While you can find several kernels on kaggle already, I highly recommend you start your ownsolution from scratch. For this homework, only use wine from the United States (only 42% of data). Feel free to subsample the data for building your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Bag of Words and simple Features [50pts]\n",
    "\n",
    "1.1 Create a baseline model for predicting wine quality using only non-text features.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>taster_twitter_handle</th>\n",
       "      <th>title</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Aromas include tropical fruit, broom, brimston...</td>\n",
       "      <td>Vulkà Bianco</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sicily &amp; Sardinia</td>\n",
       "      <td>Etna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>@kerinokeefe</td>\n",
       "      <td>Nicosia 2013 Vulkà Bianco  (Etna)</td>\n",
       "      <td>White Blend</td>\n",
       "      <td>Nicosia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "      <td>Avidagos</td>\n",
       "      <td>87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Douro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>@vossroger</td>\n",
       "      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n",
       "      <td>Portuguese Red</td>\n",
       "      <td>Quinta dos Avidagos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>@paulgwine</td>\n",
       "      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n",
       "      <td>Pinot Gris</td>\n",
       "      <td>Rainstorm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country                                        description   designation  \\\n",
       "0     Italy  Aromas include tropical fruit, broom, brimston...  Vulkà Bianco   \n",
       "1  Portugal  This is ripe and fruity, a wine that is smooth...      Avidagos   \n",
       "2        US  Tart and snappy, the flavors of lime flesh and...           NaN   \n",
       "\n",
       "   points  price           province           region_1           region_2  \\\n",
       "0      87    NaN  Sicily & Sardinia               Etna                NaN   \n",
       "1      87   15.0              Douro                NaN                NaN   \n",
       "2      87   14.0             Oregon  Willamette Valley  Willamette Valley   \n",
       "\n",
       "     taster_name taster_twitter_handle  \\\n",
       "0  Kerin O’Keefe          @kerinokeefe   \n",
       "1     Roger Voss            @vossroger   \n",
       "2   Paul Gregutt           @paulgwine    \n",
       "\n",
       "                                           title         variety  \\\n",
       "0              Nicosia 2013 Vulkà Bianco  (Etna)     White Blend   \n",
       "1  Quinta dos Avidagos 2011 Avidagos Red (Douro)  Portuguese Red   \n",
       "2  Rainstorm 2013 Pinot Gris (Willamette Valley)      Pinot Gris   \n",
       "\n",
       "                winery  \n",
       "0              Nicosia  \n",
       "1  Quinta dos Avidagos  \n",
       "2            Rainstorm  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "x = pd.read_csv('winemag-data-130k-v2.csv', index_col=0)\n",
    "x.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 129971 entries, 0 to 129970\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   country                129908 non-null  object \n",
      " 1   description            129971 non-null  object \n",
      " 2   designation            92506 non-null   object \n",
      " 3   points                 129971 non-null  int64  \n",
      " 4   price                  120975 non-null  float64\n",
      " 5   province               129908 non-null  object \n",
      " 6   region_1               108724 non-null  object \n",
      " 7   region_2               50511 non-null   object \n",
      " 8   taster_name            103727 non-null  object \n",
      " 9   taster_twitter_handle  98758 non-null   object \n",
      " 10  title                  129971 non-null  object \n",
      " 11  variety                129970 non-null  object \n",
      " 12  winery                 129971 non-null  object \n",
      "dtypes: float64(1), int64(1), object(11)\n",
      "memory usage: 13.9+ MB\n"
     ]
    }
   ],
   "source": [
    "x.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the requirements, we drop the text-based column. Moreover, since we are working on a baseline model, we shall drop the _region_2_ column, for it has high percentage of missing values, then, we'll drop the rows that contains of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.drop(columns=['description', 'region_2'], inplace=True)\n",
    "x.dropna(inplace=True)\n",
    "y = x['points']\n",
    "x.drop(columns=['points'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,\n",
       "         93,  94,  95,  96,  97,  98,  99, 100], dtype=int64),\n",
       " array([ 133,  179,  470,  904, 1746, 2786, 3999, 5799, 6958, 5398, 6713,\n",
       "        5668, 4816, 3450, 1985,  738,  231,   80,   34,   12,    7],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data isn't balanced, so we'll use SMOTE to balance it, before we move forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = x.dtypes == object\n",
    "x.loc[:, cat] = x.loc[:, cat].apply(lambda z: z.astype('category').cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "x_train, y_train = SMOTE().fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "base_model = make_pipeline(StandardScaler(), LR()).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          80       0.02      0.40      0.03        30\n",
      "          81       0.02      0.16      0.03        37\n",
      "          82       0.02      0.14      0.04        94\n",
      "          83       0.05      0.03      0.04       173\n",
      "          84       0.07      0.10      0.08       367\n",
      "          85       0.07      0.09      0.08       572\n",
      "          86       0.14      0.14      0.14       817\n",
      "          87       0.19      0.09      0.12      1152\n",
      "          88       0.16      0.09      0.12      1372\n",
      "          89       0.18      0.22      0.20      1077\n",
      "          90       0.19      0.12      0.15      1287\n",
      "          91       0.20      0.10      0.13      1143\n",
      "          92       0.19      0.08      0.11       967\n",
      "          93       0.15      0.15      0.15       700\n",
      "          94       0.14      0.16      0.15       404\n",
      "          95       0.06      0.11      0.08       144\n",
      "          96       0.06      0.13      0.09        55\n",
      "          97       0.00      0.06      0.01        17\n",
      "          98       0.01      0.29      0.02         7\n",
      "          99       0.01      0.17      0.02         6\n",
      "         100       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.12     10422\n",
      "   macro avg       0.09      0.13      0.09     10422\n",
      "weighted avg       0.16      0.12      0.13     10422\n",
      "\n",
      "AUC = 0.7709848065945695\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, plot_roc_curve, plot_confusion_matrix\n",
    "print(classification_report(y_test, base_model.predict(x_test)))\n",
    "print(f\"AUC = {roc_auc_score(y_test, base_model.predict_proba(x_test), multi_class='ovr')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not good at all. We expect to get a model to could do way better than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Create a simple text-based model using a bag-of-words approach and a linear model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "x = pd.read_csv('winemag-data-130k-v2.csv', index_col=0)\n",
    "to_remove = x.columns.to_list()\n",
    "to_remove.remove('points')\n",
    "to_remove.remove('description')\n",
    "x.drop(columns=to_remove, inplace=True)\n",
    "y = x['points']\n",
    "x.drop(columns=['points'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "x_train = vect.fit_transform(x_train['description'].to_list())\n",
    "x_test = vect.transform(x_test['description'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "x_train, y_train = SMOTE().fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "lr = LR().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          80       0.25      0.16      0.20        82\n",
      "          81       0.15      0.11      0.13       145\n",
      "          82       0.22      0.22      0.22       347\n",
      "          83       0.21      0.17      0.18       656\n",
      "          84       0.26      0.28      0.27      1284\n",
      "          85       0.24      0.23      0.23      1881\n",
      "          86       0.24      0.23      0.23      2475\n",
      "          87       0.28      0.29      0.29      3399\n",
      "          88       0.25      0.24      0.24      3430\n",
      "          89       0.22      0.19      0.20      2500\n",
      "          90       0.26      0.25      0.25      3114\n",
      "          91       0.21      0.20      0.21      2289\n",
      "          92       0.21      0.23      0.22      1854\n",
      "          93       0.19      0.19      0.19      1328\n",
      "          94       0.17      0.18      0.17       749\n",
      "          95       0.10      0.15      0.12       280\n",
      "          96       0.01      0.02      0.02       100\n",
      "          97       0.05      0.08      0.06        50\n",
      "          98       0.02      0.05      0.03        21\n",
      "          99       0.00      0.00      0.00         6\n",
      "         100       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.23     25995\n",
      "   macro avg       0.17      0.17      0.17     25995\n",
      "weighted avg       0.23      0.23      0.23     25995\n",
      "\n",
      "AUC = 0.7790112499179167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "print(classification_report(y_test, lr.predict(x_test)))\n",
    "print(f\"AUC = {roc_auc_score(y_test, lr.predict_proba(x_test), multi_class='ovr')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Try using n-grams, characters, tf-idf rescaling and possibly other ways to tune the BoW model. Be aware that you might need to adjust the (regularization of the) linear model for different feature sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from imblearn.over_sampling import SMOTE\n",
    "x = pd.read_csv('winemag-data-130k-v2.csv', index_col=0)\n",
    "to_remove = x.columns.to_list()\n",
    "to_remove.remove('points')\n",
    "to_remove.remove('description')\n",
    "x.drop(columns=to_remove, inplace=True)\n",
    "y = x['points']\n",
    "x.drop(columns=['points'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = make_pipeline(CountVectorizer(stop_words='english'), TfidfTransformer()).fit(x_train['description'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = transformer.transform(x_train['description'].to_list())\n",
    "x_test = transformer.transform(x_test['description'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = SMOTE().fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "lr = LR().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matan\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Matan\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Matan\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          80       0.13      0.26      0.17        78\n",
      "          81       0.08      0.25      0.12       124\n",
      "          82       0.15      0.26      0.19       389\n",
      "          83       0.13      0.26      0.17       602\n",
      "          84       0.19      0.22      0.20      1284\n",
      "          85       0.22      0.24      0.23      1885\n",
      "          86       0.24      0.19      0.21      2474\n",
      "          87       0.28      0.22      0.25      3335\n",
      "          88       0.27      0.20      0.23      3473\n",
      "          89       0.23      0.22      0.22      2449\n",
      "          90       0.27      0.20      0.23      3133\n",
      "          91       0.22      0.21      0.22      2295\n",
      "          92       0.24      0.22      0.23      2000\n",
      "          93       0.18      0.22      0.20      1272\n",
      "          94       0.15      0.29      0.20       735\n",
      "          95       0.10      0.27      0.14       293\n",
      "          96       0.02      0.06      0.03       109\n",
      "          97       0.08      0.12      0.09        43\n",
      "          98       0.00      0.00      0.00        13\n",
      "          99       0.00      0.00      0.00         6\n",
      "         100       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.21     25995\n",
      "   macro avg       0.15      0.19      0.16     25995\n",
      "weighted avg       0.23      0.21      0.22     25995\n",
      "\n",
      "AUC = 0.8253927867372297\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "print(classification_report(y_test, lr.predict(x_test)))\n",
    "print(f\"AUC = {roc_auc_score(y_test, lr.predict_proba(x_test), multi_class='ovr')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "lr = LR().fit(normalize(x_train, norm='l2'), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          80       0.13      0.24      0.17        78\n",
      "          81       0.08      0.21      0.11       124\n",
      "          82       0.15      0.28      0.20       389\n",
      "          83       0.13      0.21      0.16       602\n",
      "          84       0.19      0.24      0.21      1284\n",
      "          85       0.22      0.21      0.22      1885\n",
      "          86       0.23      0.21      0.22      2474\n",
      "          87       0.27      0.23      0.25      3335\n",
      "          88       0.25      0.20      0.22      3473\n",
      "          89       0.23      0.23      0.23      2449\n",
      "          90       0.27      0.20      0.23      3133\n",
      "          91       0.22      0.24      0.23      2295\n",
      "          92       0.24      0.22      0.23      2000\n",
      "          93       0.20      0.23      0.21      1272\n",
      "          94       0.16      0.26      0.20       735\n",
      "          95       0.10      0.22      0.14       293\n",
      "          96       0.05      0.06      0.05       109\n",
      "          97       0.06      0.12      0.08        43\n",
      "          98       0.00      0.00      0.00        13\n",
      "          99       0.00      0.00      0.00         6\n",
      "         100       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.22     25995\n",
      "   macro avg       0.15      0.18      0.16     25995\n",
      "weighted avg       0.23      0.22      0.22     25995\n",
      "\n",
      "AUC = 0.82494274257981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matan\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Matan\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Matan\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, lr.predict(x_test)))\n",
    "print(f\"AUC = {roc_auc_score(y_test, lr.predict_proba(x_test), multi_class='ovr')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count =  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count =  4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, average_precision_score, recall_score, classification_report, roc_auc_score\n",
    "results = np.zeros((4, 4))\n",
    "count = 0\n",
    "for i in [(1, 1), (1, 2), (2, 2), (2, 3)]:\n",
    "    transformer = CountVectorizer(stop_words='english', ngram_range=i).fit(x_train['description'].to_list())\n",
    "    x_train = transformer.transform(x_train['description'].to_list())\n",
    "    x_test = transformer.transform(x_test['description'].to_list())\n",
    "    x_train, y_train = SMOTE().fit_resample(x_train, y_train)\n",
    "    lr = LR().fit(x_train, y_train)\n",
    "    results[count, 0] = lr.score(x_test, y_test)\n",
    "    results[count, 1] = roc_auc_score(y_test, lr.predict_proba(x_test), multi_class='ovr')\n",
    "    results[count, 2] = f1_score(y_test, lr.predict(x_test), average='macro')\n",
    "    results[count, 3] = recall_score(y_test, lr.predict(x_test), average='macro')\n",
    "    count += 1\n",
    "    print('count = ', count)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>auc</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1, 1)</th>\n",
       "      <td>0.236622</td>\n",
       "      <td>0.782689</td>\n",
       "      <td>0.174187</td>\n",
       "      <td>0.174153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, 2)</th>\n",
       "      <td>0.303943</td>\n",
       "      <td>0.792835</td>\n",
       "      <td>0.208129</td>\n",
       "      <td>0.208173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2, 2)</th>\n",
       "      <td>0.283285</td>\n",
       "      <td>0.749124</td>\n",
       "      <td>0.186470</td>\n",
       "      <td>0.194465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2, 3)</th>\n",
       "      <td>0.276976</td>\n",
       "      <td>0.757223</td>\n",
       "      <td>0.173667</td>\n",
       "      <td>0.198477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy       auc        f1    recall\n",
       "(1, 1)  0.236622  0.782689  0.174187  0.174153\n",
       "(1, 2)  0.303943  0.792835  0.208129  0.208173\n",
       "(2, 2)  0.283285  0.749124  0.186470  0.194465\n",
       "(2, 3)  0.276976  0.757223  0.173667  0.198477"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=results, index=[(1, 1), (1, 2), (2, 2), (2, 3)], columns=['accuracy', 'auc', 'f1', 'recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matan\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:502: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n",
      "C:\\Users\\Matan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "transformer = CountVectorizer(stop_words='english', ngram_range=(2, 5), analyzer='char_wb').fit(x_train['description'].to_list())\n",
    "x_train = transformer.transform(x_train['description'].to_list())\n",
    "x_test = transformer.transform(x_test['description'].to_list())\n",
    "x_train, y_train = SMOTE().fit_resample(x_train, y_train)\n",
    "lr = LR().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          80       0.39      0.08      0.14        83\n",
      "          81       0.17      0.10      0.12       144\n",
      "          82       0.19      0.25      0.21       370\n",
      "          83       0.16      0.13      0.14       598\n",
      "          84       0.27      0.24      0.25      1352\n",
      "          85       0.22      0.20      0.21      1928\n",
      "          86       0.24      0.23      0.23      2497\n",
      "          87       0.26      0.30      0.28      3401\n",
      "          88       0.23      0.24      0.24      3479\n",
      "          89       0.20      0.15      0.17      2442\n",
      "          90       0.23      0.31      0.27      3023\n",
      "          91       0.22      0.19      0.20      2244\n",
      "          92       0.22      0.20      0.21      1907\n",
      "          93       0.20      0.20      0.20      1280\n",
      "          94       0.16      0.15      0.15       755\n",
      "          95       0.10      0.13      0.11       307\n",
      "          96       0.04      0.04      0.04       112\n",
      "          97       0.08      0.08      0.08        52\n",
      "          98       0.00      0.00      0.00         9\n",
      "          99       0.00      0.00      0.00         9\n",
      "         100       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.23     25995\n",
      "   macro avg       0.17      0.15      0.16     25995\n",
      "weighted avg       0.22      0.23      0.22     25995\n",
      "\n",
      "AUC = 0.8086954765823677\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, lr.predict(x_test)))\n",
    "print(f\"AUC = {roc_auc_score(y_test, lr.predict_proba(x_test), multi_class='ovr')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mid Q summary:\n",
    "Up to this point, we went easy on ourselves by not cross-validating anything, nor giving much care about the pre-processing.\n",
    "However, we'll take care of it all in the next part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Combine the non-text features and the text features. How does adding those features improve upon just using bag-of-words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>taster_twitter_handle</th>\n",
       "      <th>title</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>@paulgwine</td>\n",
       "      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n",
       "      <td>Pinot Gris</td>\n",
       "      <td>Rainstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n",
       "      <td>Reserve Late Harvest</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>Lake Michigan Shore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alexander Peartree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St. Julian 2013 Reserve Late Harvest Riesling ...</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>St. Julian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Much like the regular bottling from 2012, this...</td>\n",
       "      <td>Vintner's Reserve Wild Child Block</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>@paulgwine</td>\n",
       "      <td>Sweet Cheeks 2012 Vintner's Reserve Wild Child...</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Sweet Cheeks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "2  Tart and snappy, the flavors of lime flesh and...   \n",
       "3  Pineapple rind, lemon pith and orange blossom ...   \n",
       "4  Much like the regular bottling from 2012, this...   \n",
       "\n",
       "                          designation  price  province             region_1  \\\n",
       "2                                 NaN   14.0    Oregon    Willamette Valley   \n",
       "3                Reserve Late Harvest   13.0  Michigan  Lake Michigan Shore   \n",
       "4  Vintner's Reserve Wild Child Block   65.0    Oregon    Willamette Valley   \n",
       "\n",
       "            region_2         taster_name taster_twitter_handle  \\\n",
       "2  Willamette Valley        Paul Gregutt           @paulgwine    \n",
       "3                NaN  Alexander Peartree                   NaN   \n",
       "4  Willamette Valley        Paul Gregutt           @paulgwine    \n",
       "\n",
       "                                               title     variety        winery  \n",
       "2      Rainstorm 2013 Pinot Gris (Willamette Valley)  Pinot Gris     Rainstorm  \n",
       "3  St. Julian 2013 Reserve Late Harvest Riesling ...    Riesling    St. Julian  \n",
       "4  Sweet Cheeks 2012 Vintner's Reserve Wild Child...  Pinot Noir  Sweet Cheeks  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "x = pd.read_csv('winemag-data-130k-v2.csv', index_col=0)\n",
    "x = x.loc[x['country'] == 'US']\n",
    "y = x['points']\n",
    "x.drop(columns=['points', 'country'], inplace=True)\n",
    "x.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.loc[x['region_1'].isna()] = x.loc[x['region_1'].isna()].assign(region_1 = 'Nan')\n",
    "x.loc[x['region_2'].isna()] = x.loc[x['region_2'].isna()].assign(region_2 = 'Nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = x['taster_name'].unique()\n",
    "for n in names:\n",
    "    sub_df = x.loc[x['taster_name'] == n]['taster_twitter_handle']\n",
    "    if sub_df.isna().sum() == len(sub_df):\n",
    "        x.loc[(x['taster_name'] == n) & x['taster_twitter_handle'].isna(), 'taster_twitter_handle'] = 'Nan'\n",
    "        continue\n",
    "    elif sub_df.isna().sum() == 0:\n",
    "        continue\n",
    "    new_t = sub_df[~sub_df.isna()].iloc[0]\n",
    "    x.loc[(x['taster_name'] == n) & x['taster_twitter_handle'].isna(), 'taster_twitter_handle'] = new_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = x['taster_twitter_handle'].unique()\n",
    "for n in names:\n",
    "    sub_df = x.loc[x['taster_twitter_handle'] == n]['taster_name']\n",
    "    if sub_df.isna().sum() == len(sub_df):\n",
    "        x.loc[(x['taster_twitter_handle'] == n) & x['taster_name'].isna(), 'taster_name'] = 'Nan'\n",
    "        continue\n",
    "    elif sub_df.isna().sum() == 0:\n",
    "        continue\n",
    "    new_t = sub_df[~sub_df.isna()].iloc[0]\n",
    "    x.loc[(x['taster_twitter_handle'] == n) & x['taster_name'].isna(), 'taster_name'] = new_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.loc[x['taster_name'].isna(), 'taster_name'] = 'Nan'\n",
    "x.loc[x['taster_twitter_handle'].isna(), 'taster_twitter_handle'] = 'Nan'\n",
    "x.loc[x['designation'].isna(), 'designation'] = 'Nan'\n",
    "x.loc[x['price'].isna(), 'price'] = np.mean(x['price'].loc[~x['price'].isna()].astype('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 54504 entries, 2 to 129967\n",
      "Data columns (total 11 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   description            54504 non-null  object \n",
      " 1   designation            54504 non-null  object \n",
      " 2   price                  54504 non-null  float64\n",
      " 3   province               54504 non-null  object \n",
      " 4   region_1               54504 non-null  object \n",
      " 5   region_2               54504 non-null  object \n",
      " 6   taster_name            54504 non-null  object \n",
      " 7   taster_twitter_handle  54504 non-null  object \n",
      " 8   title                  54504 non-null  object \n",
      " 9   variety                54504 non-null  object \n",
      " 10  winery                 54504 non-null  object \n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 5.0+ MB\n"
     ]
    }
   ],
   "source": [
    "x.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = x.dtypes == object\n",
    "cat['description'] = False\n",
    "x.loc[:, cat] = x.loc[:, cat].apply(lambda z: z.astype('category').cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Word Vectors [50pts]\n",
    "\n",
    "Use a pretrained word-embedding (word2vec, glove or fasttext) for featurization instead of the bag-of-words model. Does this improve classification? How about combining the embedded words with the BoW model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 Transformers (bonus / optional) [50pts] - Like Extra Assignment with 50% weight\n",
    "\n",
    "Fine-tune a BERT model on the text data alone using the transformers library. How does this model compare to a BoW model, and how does it compare to a model using all features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
